{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80dc10e4",
   "metadata": {},
   "source": [
    "# This is an example file to show how to train the model using the Rg.npy dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307241cc",
   "metadata": {},
   "source": [
    "### Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a82576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from numpy import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "\n",
    "# Import regression models\n",
    "from sklearn.svm import SVR\n",
    "# from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "# from sklearn.linear_model import Ridge\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ced61",
   "metadata": {},
   "source": [
    "# Load the data and create arrays for Rg and sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.load(\"Rg_data.npy\",allow_pickle=True)\n",
    "seq = seq.item()\n",
    "data_seq = []\n",
    "Rg = []\n",
    "nu = []\n",
    "for i in range(len(seq)):\n",
    "    data_seq.append(seq[i][0])\n",
    "    Rg.append(seq[i][1][1][1])\n",
    "    nu.append(seq[i][1][1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d3f1e",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c97f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = utils.seqs_to_count_encoding(data_seq)\n",
    "OE = utils.seqs_to_ordinal_encoding(data_seq)\n",
    "OHE = utils.seqs_to_onehot(data_seq)\n",
    "BAA = utils.seqs_to_bag_of_AAs(data_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e54801",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rg = np.array(Rg).reshape(-1,1)\n",
    "nu = np.array(nu).reshape(-1,1)\n",
    "\n",
    "X = CE # Choose the features\n",
    "Y = np.hstack((Rg,nu)) # stack Rg and nu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1af83c",
   "metadata": {},
   "source": [
    "# Define the dataset split\n",
    "Give the random seed, cross validation fold and learning curve split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 6\n",
    "seed = 10\n",
    "split = 8\n",
    "c, CL = utils.get_CL_from_OE(OE)\n",
    "\n",
    "Train_indices,Test_indices = utils.CV_split_CL(fold,seed,c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4dd4cc",
   "metadata": {},
   "source": [
    "# Get the best parameters\n",
    "Here, we do the hyperparameter tuning. This function needs the following information: which regression model, X, Y, Train_indices, Test_indices, Fold of cross validation, Parameter list, Whether to train Rg or not(default is train the Rg: Train_Rg = True)\n",
    "\n",
    "The parameter list setup is as follow:\n",
    "LRR: A list of range that you want to test the parameter, alpha.\n",
    "KRR: A list consists of two sublists. The first one is for alpha, and the second one is for gamma.\n",
    "SVR: A list consists of three sublists. The first one is for C, the second one is for gamma, and the third is for alpha.\n",
    "GPR: A list of range that you want to test the parameter, alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d041d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameter_list = []\n",
    "C_range = [1, 10, 100, 1000]\n",
    "G_range = [0.001,0.01,0.1,1]\n",
    "E_range = [0.001, 0.01, 0.1, 1]\n",
    "Parameter_list.append(C_range)\n",
    "Parameter_list.append(G_range)\n",
    "Parameter_list.append(E_range)\n",
    "\n",
    "best_C, best_gamma, best_epsilon = utils.Hyperparameters_Tuning(\"SVR\", X, Y, Train_indices, Test_indices, fold, Parameter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90a294",
   "metadata": {},
   "source": [
    "# Learning curve train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef25217",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_indices,Test_indices = utils.LC_split_CL(fold,split,seed,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a7edf",
   "metadata": {},
   "source": [
    "## Define a model using the best parameters and obtain the learning curve\n",
    "The LC_results will give a list containing train loss, validation loss, train size, validation size, train score and validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aad975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel=\"rbf\", C = best_C, gamma = best_gamma, epsilon = best_epsilon)\n",
    "LC_results = utils.Learning_curve(model, X, Y, Train_indices, Test_indices, fold, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67f9af",
   "metadata": {},
   "source": [
    "# Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test based on indices\n",
    "\n",
    "X_train_unscaled = []\n",
    "X_test_unscaled = []\n",
    "Y_train_unscaled = []\n",
    "Y_test_unscaled = []\n",
    "\n",
    "for i in Train_indices:\n",
    "    X_train_unscaled.append(X[i])\n",
    "    Y_train_unscaled.append(Y[i])\n",
    "for i in Test_indices:\n",
    "    X_test_unscaled.append(X[i])\n",
    "    Y_test_unscaled.append(Y[i])\n",
    "    \n",
    "X_train_unscaled = np.vstack(X_train_unscaled)\n",
    "X_test_unscaled = np.vstack(X_test_unscaled)\n",
    "Y_train_unscaled = np.vstack(Y_train_unscaled)\n",
    "Y_test_unscaled = np.vstack(Y_test_unscaled)\n",
    "\n",
    "Y_train_Rg = Y_train_unscaled[:,0]\n",
    "Y_test_Rg = Y_test_unscaled[:,0]\n",
    "\n",
    "# Normalize input\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_unscaled)\n",
    "X_train = scaler.transform(X_train_unscaled)\n",
    "X_test = scaler.transform(X_test_unscaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "svr_rbf = SVR(kernel=\"rbf\", C = best_C, gamma = best_gamma, epsilon = best_epsilon)\n",
    "svr_rbf.fit(X_train,Y_train_Rg)\n",
    "svr_rbf.score(X_test,Y_test_Rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e333e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the prediction and the test performance\n",
    "\n",
    "Y_test_pred = svr_rbf.predict(X_test)\n",
    "print(utils.coeff_determination(Y_test_Rg,Y_test_pred))\n",
    "print(utils.percent_error(Y_test_Rg,Y_test_pred))\n",
    "print(utils.MSE(Y_test_Rg,Y_test_pred))\n",
    "print(utils.RMSE(Y_test_Rg,Y_test_pred))\n",
    "print(utils.MAE(Y_test_Rg,Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b0841",
   "metadata": {},
   "source": [
    "# Extrapolation Test\n",
    "\n",
    "The extrapolation test is implemented as follow:\n",
    "1) Define a variable that contains the best model parameters\n",
    "<br>\n",
    "2) Give the function X, Y, Train_indices, Test_indices, model and binary variable forward(default is True).\n",
    "\n",
    "The results will give a list containing training size, test loss and test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b86449",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel=\"rbf\", C = best_C, gamma = best_gamma, epsilon = best_epsilon)\n",
    "results = utils.extrapolation_test_classical_regression(X, Y, Train_indices, Test_indices, model, forward = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
