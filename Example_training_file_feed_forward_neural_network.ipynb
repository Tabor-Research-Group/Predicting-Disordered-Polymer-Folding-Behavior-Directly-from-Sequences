{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08f083a",
   "metadata": {},
   "source": [
    "# This is an example file to show how to train the model using the Rg.npy dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176139a4",
   "metadata": {},
   "source": [
    "### Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a82576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from numpy import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7d5e5",
   "metadata": {},
   "source": [
    "# Load the data and create arrays for Rg and sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.load(\"Rg_data.npy\",allow_pickle=True)\n",
    "seq = seq.item()\n",
    "data_seq = []\n",
    "Rg = []\n",
    "nu = []\n",
    "for i in range(len(seq)):\n",
    "    data_seq.append(seq[i][0])\n",
    "    Rg.append(seq[i][1][1][1])\n",
    "    nu.append(seq[i][1][1][3])\n",
    "OE = utils.seqs_to_ordinal_encoding(data_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e54801",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rg = np.array(Rg).reshape(-1,1)\n",
    "nu = np.array(nu).reshape(-1,1)\n",
    "X = utils.seqs_to_bag_of_AAs(data_seq)\n",
    "\n",
    "Y = np.hstack((Rg,nu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb33ddf",
   "metadata": {},
   "source": [
    "# Define the dataset split random seed, six-fold cross validation and learning curve split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 6\n",
    "seed = 10\n",
    "split = 8\n",
    "c, CL = utils.get_CL_from_OE(OE)\n",
    "Train_indices,Test_indices = utils.LC_split_CL(fold,split,seed,c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b84912",
   "metadata": {},
   "source": [
    "## Using the train indices and test indices to separate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test based on indices\n",
    "\n",
    "X_train_unscaled = []\n",
    "X_test_unscaled = []\n",
    "Y_train_unscaled = []\n",
    "Y_test_unscaled = []\n",
    "\n",
    "for i in Train_indices:\n",
    "    X_train_unscaled.append(X[i])\n",
    "    Y_train_unscaled.append(Y[i])\n",
    "for i in Test_indices:\n",
    "    X_test_unscaled.append(X[i])\n",
    "    Y_test_unscaled.append(Y[i])\n",
    "    \n",
    "X_train_unscaled = np.vstack(X_train_unscaled)\n",
    "X_test_unscaled = np.vstack(X_test_unscaled)\n",
    "Y_train_unscaled = np.vstack(Y_train_unscaled)\n",
    "Y_test_unscaled = np.vstack(Y_test_unscaled)\n",
    "\n",
    "# Normalize input\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_unscaled)\n",
    "X_train = scaler.transform(X_train_unscaled)\n",
    "X_test = scaler.transform(X_test_unscaled)\n",
    "\n",
    "scalerY= MinMaxScaler()\n",
    "scalerY.fit(Y_train_unscaled)\n",
    "Y_train = scalerY.transform(Y_train_unscaled)\n",
    "Y_test = scalerY.transform(Y_test_unscaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541829b",
   "metadata": {},
   "source": [
    "# Training a feedfoward neural network using Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30035d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard architecture choices\n",
    "n_batch = 32\n",
    "patience = 25\n",
    "n_hidden_nodes = 100\n",
    "n_hidden_layers = 2\n",
    "n_epoch = 10**4\n",
    "tf.random.set_seed(1)\n",
    "learning_rate = 0.0001 # learning rate\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', \n",
    "               mode = 'min', verbose = 1, \n",
    "               patience = patience,restore_best_weights=True) # patience for early stopping\n",
    "\n",
    "# Choose optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Find input and output dimensions\n",
    "input_dim = np.shape(X_train)[1]\n",
    "output_dim = 2\n",
    "\n",
    "# Create DNN\n",
    "DNN = utils.create_DNN(input_dim, output_dim, n_hidden_nodes, n_hidden_layers)\n",
    "\n",
    "# Compile DNN (and choose loss)\n",
    "DNN.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "# Train DNN\n",
    "DNN.fit(X_train, Y_train, epochs=n_epoch, batch_size=n_batch, \n",
    "                shuffle=True, callbacks = [es], \n",
    "                validation_split=0.25)\n",
    "\n",
    "# Calculate training, validaiton, and testing loss\n",
    "train_loss = np.asarray(DNN.history.history['loss'])\n",
    "val_loss = np.asarray(DNN.history.history['val_loss'])\n",
    "\n",
    "DNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = DNN.predict(X_test)\n",
    "Y_test_pred = scalerY.inverse_transform(Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe78473",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utils.coeff_determination(Y_test_unscaled,Y_test_pred))\n",
    "print(utils.percent_error(Y_test_unscaled,Y_test_pred))\n",
    "print(utils.MAE(Y_test_unscaled,Y_test_pred))\n",
    "print(utils.MSE(Y_test_unscaled,Y_test_pred))\n",
    "print(utils.RMSE(Y_test_unscaled,Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428aa3e",
   "metadata": {},
   "source": [
    "# Extrapolation Test\n",
    "The extrapolation test is implemented as follow:\n",
    "\n",
    "1) Define a variable that contains the model parameters\n",
    "2) Give the function X, Y, Train_indices, Test_indices, model and binary variable forward(default is True).\n",
    "\n",
    "The results will give a list containing training size, test loss and test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eafdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard architecture choices\n",
    "n_batch = 32\n",
    "patience = 25\n",
    "n_hidden_nodes = 100\n",
    "n_hidden_layers = 2\n",
    "n_epoch = 10**4\n",
    "tf.random.set_seed(1)\n",
    "learning_rate = 0.0001 # learning rate\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', \n",
    "               mode = 'min', verbose = 1, \n",
    "               patience = patience,restore_best_weights=True) # patience for early stopping\n",
    "\n",
    "# Choose optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Find input and output dimensions\n",
    "input_dim = np.shape(X_train)[1]\n",
    "output_dim = 2\n",
    "\n",
    "# Create DNN\n",
    "DNN = utils.create_DNN(input_dim, output_dim, n_hidden_nodes, n_hidden_layers)\n",
    "\n",
    "# Compile DNN (and choose loss)\n",
    "DNN.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "results = utils.extrapolation_test_DNN(X, Y, Train_indices, Test_indices, DNN, forward = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d57453",
   "metadata": {},
   "source": [
    "# Training on temperture data\n",
    "\n",
    "In this section, the temperature is incorporated into the feature and can be used to train the model.\n",
    "The preparation process is slightly different and be written as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db22eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for Training on temperature data preparation\n",
    "\n",
    "seq = np.load(\"Rg_data.npy\",allow_pickle=True)\n",
    "seq = seq.item()\n",
    "data_seq = []\n",
    "Rg = []\n",
    "nu = []\n",
    "Temp = []\n",
    "for i in range(len(seq)):\n",
    "    data_seq.append(seq[i][0])\n",
    "    for j in range(len(seq[i][1])):\n",
    "        Temp.append(seq[i][1][j][0])\n",
    "        Rg.append(seq[i][1][j][1])\n",
    "        nu.append(seq[i][1][j][3])\n",
    "N_of_Temp = len(seq[i][1])\n",
    "OE = utils.seqs_to_ordinal_encoding(data_seq)\n",
    "\n",
    "Rg = np.array(Rg).reshape(-1,1)\n",
    "nu = np.array(nu).reshape(-1,1)\n",
    "X = utils.seqs_to_bag_of_AAs(data_seq)\n",
    "\n",
    "Y = np.hstack((Rg,nu))\n",
    "Train_indices = [item for sublist in Train_indices for item in sublist] # flatten the train indices\n",
    "\n",
    "X_train_unscaled = []\n",
    "X_test_unscaled = []\n",
    "Y_train_unscaled = []\n",
    "Y_test_unscaled = []\n",
    "\n",
    "for i in Train_indices:\n",
    "    for j in range(N_of_Temp):\n",
    "        X_tmp = np.hstack((X[i],Temp[i*N_of_Temp+j]))\n",
    "        Y_tmp = Y[i*N_of_Temp+j]\n",
    "        X_train_unscaled.append(X_tmp)\n",
    "        Y_train_unscaled.append(Y_tmp)\n",
    "for i in Test_indices:\n",
    "    for j in range(N_of_Temp):\n",
    "        X_tmp = np.hstack((X[i],Temp[i*N_of_Temp+j]))\n",
    "        Y_tmp = Y[i*N_of_Temp+j]\n",
    "        X_test_unscaled.append(X_tmp)\n",
    "        Y_test_unscaled.append(Y_tmp)\n",
    "    \n",
    "X_train_unscaled = np.vstack(X_train_unscaled)\n",
    "X_test_unscaled = np.vstack(X_test_unscaled)\n",
    "Y_train_unscaled = np.vstack(Y_train_unscaled)\n",
    "Y_test_unscaled = np.vstack(Y_test_unscaled)\n",
    "\n",
    "Y_train_nu = Y_train_unscaled[:,0]\n",
    "Y_test_nu = Y_test_unscaled[:,0]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_unscaled)\n",
    "X_train = scaler.transform(X_train_unscaled)\n",
    "X_test = scaler.transform(X_test_unscaled)\n",
    "\n",
    "scalerY = MinMaxScaler()\n",
    "scalerY.fit(Y_train_unscaled)\n",
    "Y_train = scalerY.transform(Y_train_unscaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
